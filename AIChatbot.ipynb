{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import nltk\r\n",
                "from nltk.stem.lancaster import LancasterStemmer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "stemmer = LancasterStemmer"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import numpy \r\n",
                "import numpy as np\r\n",
                "import tflearn\r\n",
                "import tensorflow as tf\r\n",
                "import random\r\n",
                "import json"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "with open(\"intents.json\") as file:\r\n",
                "     data = json.load(file)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "words = []\r\n",
                "labels = []\r\n",
                "docs_x = []\r\n",
                "docs_y = []"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for intent in data[\"intents\"]:\r\n",
                "    for pattern in intent[\"patterns\"]:\r\n",
                "        wrds = nltk.word_tokenize(pattern)\r\n",
                "        words.extend(wrds)\r\n",
                "        docs_x.append(wrds)\r\n",
                "        docs_y.append(intent[\"tag\"])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "     \r\n",
                "    if intent[\"tag\"] not in labels:\r\n",
                "        labels.append(intent[\"tag\"])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\r\n",
                "words = sorted(list(set(words)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "labels = sorted(labels)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "training = []\r\n",
                "output = []\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "out_empty = [0 for _ in range(len(labels))]\r\n",
                "\r\n",
                "for x, doc in enumerate(docs_x):\r\n",
                "    bag = []\r\n",
                "    \r\n",
                "    wrds = [stemmer.stem(w) for w in doc]\r\n",
                "    \r\n",
                "    for w in words:\r\n",
                "        if w in wrds:\r\n",
                "            bag.append(1)\r\n",
                "        else:\r\n",
                "            bag.append(0)\r\n",
                "            \r\n",
                "    output_row = out_empty[:]\r\n",
                "    output_row[labels.index(docs_y[x])] = 1\r\n",
                "    \r\n",
                "    training.append(bag)\r\n",
                "    output.append(output_row)\r\n",
                "    \r\n",
                "training = numpy.array(training)\r\n",
                "output = np.array(output)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "tf.reset_default_graph()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "net = tflearn.input_data(shape=[None, len(training[0])])\r\n",
                "net = tflearn.fully_connected(net, 8)\r\n",
                "net = tflearn.fully_connected(net, 8)\r\n",
                "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\r\n",
                "net = tflearn.regression(net)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "model = tflearn.DNN(net)\r\n",
                "\r\n",
                "model.fit(training, output, n_epoch=2000, batch_size=8, show_metric=True)\r\n",
                "model.save(\"model.tflearn\")"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.5 64-bit"
        },
        "interpreter": {
            "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}